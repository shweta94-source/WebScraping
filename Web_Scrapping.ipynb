{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scrapping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#variables\n",
        "subHeading, subHeadingList  =([] for i in range(2))\n",
        "intro = ''\n",
        "chapterName = ''\n",
        "\n",
        "#final answer list\n",
        "subHeadingIntro, subHeadings, allEaxmples, allListItems, allImages, allTableValues =([] for i in range(6))"
      ],
      "metadata": {
        "id": "nuLXn2NEyo_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mON1xAP0wwO0"
      },
      "outputs": [],
      "source": [
        "# import files  \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import NavigableString\n",
        "import csv  \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert list into string\n",
        "def listIntoString(lists):    \n",
        "    str1 =''\n",
        "    for ele in lists:         \n",
        "        str1 += ele  \n",
        "    return str1"
      ],
      "metadata": {
        "id": "qxu5txh7y-q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here all p tags are passed which includes text all other headings also\n",
        "# so here exatracting only needed text for \n",
        "def filterSubHeadingIntro(subHeadingIntro):\n",
        "    removeStr = len(subHeadingIntro)\n",
        "    i = -1\n",
        "    subIntro = []\n",
        "    lastItem = \"\"\n",
        "    while removeStr > 0:   \n",
        "        if( len(subHeadingIntro[i]) != 0 ):\n",
        "            if(len(subIntro) == 0):\n",
        "                subIntro.append(subHeadingIntro[i])\n",
        "                lastItem = subHeadingIntro[i]\n",
        "            else:\n",
        "                subIntro.append(subHeadingIntro[i].replace(lastItem, \"\"))\n",
        "                lastItem = subHeadingIntro[i]           \n",
        "        i -= 1\n",
        "        removeStr -= 1\n",
        "    subIntro.reverse()\n",
        "    return subIntro"
      ],
      "metadata": {
        "id": "FgtmO4SYy_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findHeading(soup):    \n",
        "  # print(\"\\n==========PAGE HEADING==============\")\n",
        "  heading = soup.find_all('span', class_='color_h1')\n",
        "  if(heading != []):\n",
        "    heading = heading[0].get_text()\n",
        "  else:\n",
        "    heading = \"\"  \n",
        "  \n",
        "\n",
        "  # print(\"\\n==========INTRODUCTION==============\")\n",
        "  global intro \n",
        "  intro = soup.find_all('p', class_='intro')\n",
        "  \n",
        "  if(intro != []):\n",
        "    intro = intro[0].get_text()\n",
        "  else:\n",
        "    intro = \"\"  \n",
        "  \n",
        "  return heading"
      ],
      "metadata": {
        "id": "JVza274ty_nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findSubHeading(soup):\n",
        "  global subHeadings\n",
        "  try:\n",
        "    # print(\"\\n==========ALL SUB HEADINGS==============\")\n",
        "    sub_heading = soup.find('hr')\n",
        "    sub_heading = sub_heading.find_next_siblings(\"h2\")\n",
        "    for h in sub_heading:\n",
        "        if (h.string.find('Exercises') != -1):\n",
        "            break    \n",
        "        elif(h.string == 'Chapter Summary'):\n",
        "            break\n",
        "        elif(h.string == 'HTML List Tags'):\n",
        "            break\n",
        "        else:\n",
        "            subHeadings.append(h.get_text())\n",
        "  except:\n",
        "    subHeadings.append('')"
      ],
      "metadata": {
        "id": "KzFXps_TzAiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findIntroSubheading(soup):\n",
        "  global subHeadingIntro\n",
        "  # print(\"\\n==========INTRO FOR ALL SUB HEADINGS==============\")\n",
        "  try:\n",
        "    sub_heading_intro = soup.find('div', id='main')\n",
        "    length = len(sub_heading_intro.find_all('hr'))\n",
        "    i=0\n",
        "    while i < length:\n",
        "        sub = sub_heading_intro.find_all('hr')[i]\n",
        "        sub = sub.find_next_siblings(\"p\")\n",
        "        for k in sub:      \n",
        "            if(k.get_text() == intro):\n",
        "                continue\n",
        "            if(k.get_text().__contains__(\"W3Schools' tag reference contains\")):\n",
        "                continue\n",
        "            else:  \n",
        "                subHeadingList.append(k.get_text()) \n",
        "\n",
        "        str1 = listIntoString(subHeadingList)\n",
        "        subHeadingList.clear()  \n",
        "        if(str1 not in subHeadingIntro):\n",
        "            subHeadingIntro.append(str1)               \n",
        "        i += 1\n",
        "  except:\n",
        "    subHeadingIntro.append('')               \n",
        "  # sending data for filtering\n",
        "  subHeadingIntro = filterSubHeadingIntro(subHeadingIntro)"
      ],
      "metadata": {
        "id": "LPIVqlgxy_v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findExample(soup):\n",
        "  global allEaxmples\n",
        "  dummyExamples = []\n",
        "  text = ''\n",
        "\n",
        "  # print(\"\\n==========ALL EXAMPLES FOR SUB HEADINGS==============\")\n",
        "  try:\n",
        "    example = soup.select(\"div.w3-example div.notranslate\")\n",
        "    for e in example:    \n",
        "      example = False            \n",
        "      dummyExamples.append(e.get_text())        \n",
        "      par = e.parent     \n",
        "    \n",
        "      #under one heading can have multiple examples\n",
        "      for sibling in par.next_siblings: \n",
        "        # print(sibling)\n",
        "        for s in sibling:\n",
        "          if(str(s) == '<h3>Example</h3>'):\n",
        "            example = True\n",
        "        if(example == False):\n",
        "          if(str(sibling) == '<hr/>'):\n",
        "            dummyExamples.append(\"|||\") \n",
        "            break\n",
        "\n",
        "    # arranging all examples  \n",
        "    for e in dummyExamples:    \n",
        "      if(e == '|||'):\n",
        "        allEaxmples.append(text)\n",
        "        text = ''\n",
        "      else:\n",
        "        if(len(text) == 0):\n",
        "          text += e\n",
        "        else:\n",
        "          text += ' &&&&&& '+e \n",
        "  except:\n",
        "    allEaxmples.append(\"\")"
      ],
      "metadata": {
        "id": "HJ6d9EKFy_4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find listItems on webpage\n",
        "def findListItems(soup):\n",
        "  global allListItems\n",
        "  heading = \"\"\n",
        "  items = \"\"\n",
        "  finalItem = \"\"\n",
        "\n",
        "  try:\n",
        "    unorderedList = soup.find_all('ul')\n",
        "    for k in unorderedList:\n",
        "      heading += k.find_previous_sibling('h2').get_text() + \"###\"    #listHeading\n",
        "      items += \"&&&\"+k.get_text().replace(\"\\n\", \"|||\")               #listItems\n",
        "\n",
        "    finalItem = heading + items\n",
        "    allListItems.append(finalItem)       \n",
        "  except:\n",
        "    allListItems.append(\"\")\n",
        "  # print(allListItems)"
      ],
      "metadata": {
        "id": "zfP1PDnXvoxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find images on webpage\n",
        "def findImageUrl(soup):\n",
        "  global allImages\n",
        "\n",
        "  try:\n",
        "    content = soup.find(\"div\", id=\"main\")\n",
        "    images = content.find_all('img')\n",
        "    for img in images:\n",
        "      allImages.append(img['src'])\n",
        "  except:\n",
        "    allImages.append(\"\")"
      ],
      "metadata": {
        "id": "-Se5SKGkhLQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find all table values\n",
        "def findTableValues(soup):\n",
        "  global allTableValues\n",
        "  tableText = \"\"\n",
        "\n",
        "  try:\n",
        "    content = soup.find(\"div\", id=\"main\")\n",
        "    tables = content.find_all('table')  \n",
        "    for table in tables:\n",
        "      tableRow = table.find_all(\"tr\")\n",
        "\n",
        "      for tr in tableRow:\n",
        "        if(tr.find_all(\"th\")):\n",
        "          tableHeading = tr.find_all(\"th\")\n",
        "          for th in tableHeading:\n",
        "            tableText += th.get_text() + \"|||\"\n",
        "\n",
        "        elif(tr.find_all(\"td\")):\n",
        "          tableData = tr.find_all(\"td\")\n",
        "          for td in tableData:\n",
        "            tableText += td.get_text() + \"|||\"          \n",
        "        tableText += \"&&&\"\n",
        "\n",
        "      allTableValues.append(tableText)\n",
        "      tableText = \"\"\n",
        "  except:\n",
        "    allTableValues.append(\"\")\n",
        "  # print(allTableValues)\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "cEVEXhKI9EbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeInCSV(chapterName, heading, link, intro, subHeadings, subHeadingIntro, allEaxmples, allListItems, allTableValues, allImages):   \n",
        "\n",
        "  \n",
        "  for k in range(len(subHeadings)):\n",
        "    if(k >= len(subHeadingIntro)):\n",
        "      subHeadingIntro.append(\"\")\n",
        "    if(k >= len(allEaxmples)):\n",
        "      allEaxmples.append(\"\")\n",
        " \n",
        "\n",
        "  with open('web.csv', 'a', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    counter = 0 \n",
        "    heading = chapterName +'||'+ heading.strip() \n",
        "\n",
        "    data = heading, link, intro, subHeadings, subHeadingIntro, allEaxmples, allListItems, allTableValues, allImages\n",
        "    writer.writerow(data)\n",
        "    "
      ],
      "metadata": {
        "id": "nSOvGL4RBl7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = ['Heading', 'Link', 'Intro', 'Subheading', 'Definitions', 'Example', 'List_item', 'table_item', 'Image_url',]\n",
        "with open('web.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)  \n",
        "\n"
      ],
      "metadata": {
        "id": "GJUlkSBvyhnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webLinks = pd.read_csv('sql.txt',sep='\\n',header=None)[0].tolist()\n",
        "# webLinks = webLinks[:3]\n",
        "\n",
        "num = 1\n",
        "\n",
        "for link in webLinks:  \n",
        "#   # print(link)\n",
        "  chapterName = link.split(\"/\")[3]\n",
        "\n",
        "  r = requests.get(link)   \n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  # print(soup)\n",
        "  heading = findHeading(soup)\n",
        "  findSubHeading(soup)  \n",
        "  findIntroSubheading(soup)\n",
        "  findExample(soup)\n",
        "  findListItems(soup)\n",
        "  findImageUrl(soup)\n",
        "  findTableValues(soup)\n",
        "\n",
        "  writeInCSV(chapterName, heading, link, intro, subHeadings, subHeadingIntro, allEaxmples, allListItems, allTableValues, allImages)  \n",
        "  subHeadingIntro.clear()\n",
        "  subHeadings.clear()\n",
        "  allEaxmples.clear()  \n",
        "  allListItems.clear()\n",
        "  allImages.clear()\n",
        "  allTableValues.clear()\n",
        "  print(num, 'RECORD DONE!')\n",
        "  num += 1\n",
        "  \n",
        "  # print(chapterName)\n",
        "  # print(heading)\n",
        "  # print(link)\n",
        "  # print(intro)\n",
        "  # print(subHeadings)\n",
        "  # print(subHeadingIntro)\n",
        "  # print(allEaxmples)\n",
        "  # print(allListItems)  "
      ],
      "metadata": {
        "id": "LH1q3gzwoUT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"web.csv\")  \n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LrTSBx-jE2wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file1 = open(\"link.txt\",\"a\")\n",
        "\n",
        "# URL = \"https://www.w3schools.com/cpp/\"\n",
        "# r = requests.get(URL)   \n",
        "# soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "# divs  = soup.find(\"div\", id=\"leftmenuinnerinner\")\n",
        "# # soup.select(\"div.w3-example div.notranslate\")\n",
        "\n",
        "# for a in divs.find_all('a', href=True, target=\"_top\"):\n",
        "#   links = URL + a['href']\n",
        "#   file1.write(links)\n",
        "#   file1.write('\\n')\n",
        "#   print(links)\n",
        "\n",
        "# file1.close()"
      ],
      "metadata": {
        "id": "w9W9Fsp9w7Ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}